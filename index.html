<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1000px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Video Temporal Grounding</title>
		<meta property="og:image" content="https://richzhang.github.io/splitbrainauto/index_files/cvpr_fb_icon.png"/>
		<meta property="og:title" content=". In NeurlPS, 2025." />
  </head>

	
<body>
    <br>
          <center>
          	<!-- <span style="font-size:34px">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Video Temporal Grounding</span> -->
          	<span style="font-size:34px">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Video Temporal Grounding</span><br>
		  <br>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://lwpyh.github.io/">Jian Hu</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://zxccade.github.io/">Zixu Cheng</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=130px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://ece.hkust.edu.hk/eeguan">Isabel Guan</a></span>
							</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=FCJVUYgAAAAJ&hl=zh-CN">Jianye Hao</a></span>
							</a><sup>3</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a></span>
							</a><sup>4</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://shaokun91.github.io/">Kun Shao</a></span>
							</a><sup>3</sup></span>
		  		  		</center>
		  		  		</td>
			  </table>
          	<span style="font-size:20px"><sup>1</sup>Queen Mary University of London, <sup>2</sup>Hong Kong University of Science and Technology, <sup>3</sup>Huawei Noah's Ark Lab, <sup>4</sup>University College London</span><br>
		  <span style="font-size:20px">{jian.hu, zixu.cheng, s.gong}@qmul.ac.uk, {jianye.hao,shaokun2}@huawei.com, jun.wang@cs.ucl.ac.uk</span>
          	<!-- <span style="font-size:22px">In CVPR, 2017.</span><br> -->
          		<!-- <span style="font-size:30px">ECCV 2016.</span> -->

	  		  <table align=center width=400px>
	  			  <tr>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:20px"><a href='https://github.com/lwpyh/URPO'> Code [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:20px"><a href='https://arxiv.org/abs/2508.06317'> NeurIPS 2025 [Paper]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>
  		  <br>

	<table align=center width=800px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "motivation_shown.png" height="240px"></img>
  	                	<br>
						<figcaption style="text-align: justify";> Motivation: (a) A comparison between full-data and data-efficient adaptation in Cross-domain Temporal Grounding (CTG). Existing methods adapt on thousands of unlabelled target videos (grey), which is slow and resource-heavy. We propose a data-efficient CTG setting using only 100 or 200 randomly selected target videos. Despite the limited data, our method matches or exceeds performance on the TaCoS $\rightarrow$ ActivityNet task.
   (b) Conceptual comparison between MC Dropout and GRPO rollout. MC Dropout samples subnetworks via stochastic neuron dropout and estimates uncertainty from output diversity. GRPO rollouts similarly sample diverse structural sequences from the policy. URPA leverages this property to generate averaged pseudo labels and estimate uncertainty via rollout standard deviation, enabling uncertainty-quantified adaptation without ground-truth labels.</figcaption>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					</center>
  	              </td>
  		  </table>

  		  <br>
            	  <hr>
<table align=center width=1500px>
 <center><h1>Abstract</h1></center>
	<div align="justify">
	<p>
Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic
matching, they often struggle with fine-grained temporal
localisation. Recently, Group Relative Policy Optimisation (GRPO)
reformulates the inference process as a reinforcement learning
task, enabling fine-grained grounding and achieving strong in-domain
performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To
overcome both problems, we
introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding
method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce
Uncertainty-quantified Rollout Policy
Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using
GRPO rollouts, averages them to form a pseudo label, and estimates
confidence from the variance across these rollouts. This confidence
then weights the training rewards, guiding the model to focus on
reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. </p>
		</div>
<br>
<hr>
<!-- <hr> -->
<!-- <center><h1>Video</h1></center> -->
     <!-- <center>
     <iframe width="560" height="315" src="https://www.youtube.com/embed/oMcd6maQgQk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
     </center> -->
<!-- <br> -->

<center><h1>Framework</h1></center>
<table align="center" width="800px">
    <tr>
        <td width="400px">
            <center>
                <figure>
                    <img class="rounded" src="framework_nips.png" height="400px" />
                    <figcaption style="text-align: justify;">
                            Uncertainty-quantified Rollout Policy Adaptation (URPA):</b>
During source model training, we perform supervised GRPO training using labelled videos \( V_s \).
Specifically, the format reward \( R_{\text{format}} \) encourages the model to “think first and then answer,”
while the accuracy reward \( R_{\text{tiou}} \) aligns the predicted temporal grounding 
\( I_s^{\text{pred}} \) with the relaxed ground truth \( \tilde{I}_s^{\text{gt}} \)
for supervised learning.
In target model knowledge adaptation, we adapt the model using \( K \) unlabelled target videos.
For each video \( V_t \), we first compute the average output over \( G \) rollouts
to obtain a pseudo label \( \hat{I}_t^{\text{gt}} \).
Then, we calculate the standard deviation across these rollouts and transform it into
a confidence score \( c \) to quantify uncertainty on pseudo labels,
which is then used to weight different pseudo-labels when constructing a weighted reward function
for test-time target model adaptation.</figcaption>
                </figure>
        <td width="400px">
        </td>
    </tr>
</table>
<hr>
<table align="center" width="1700px">

<center><h1>Experiments</h1></center>
<table align="center" width="800px style="margin-bottom: -10px;">
    <tr>
        <td width="400px">
            <center>
                <figure>
			<figcaption style="text-align: justify;">
				
    </figcaption>
		    <img class="rounded" src="experiment_nips.png" height="600px" />
                </figure>
        <td width="400px">
        </td>
    </tr>
</table>
  <hr>
<table align="center" width="2800px">

<center><h1>Qualitative Evaluation</h1></center>
	<table align=center width="800px style="margin-bottom: 10px;">
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "viis_nips.png" height="500px"></img>
  	                	<br>
						<figcaption style="text-align: justify";> Qualitative Analysis on Charades → ActivityNet.</figcaption>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					</center>
  	              </td>
  		  </table>

  		  <br>
            	  <hr>
<table align=center width=1900px>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2025cos,
  title={Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Video Temporal Grounding},
  author={Hu, Jian and Cheng, Zixu and Gong, Shaogang and Guan, Isabel and Hao, Jianye and Wang, Jun and Shao, Kun},
  journal={arXiv preprint arXiv:2508.06317},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
           
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
	
</body>
</html>
