<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1000px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</title>
		<meta property="og:image" content="https://richzhang.github.io/splitbrainauto/index_files/cvpr_fb_icon.png"/>
		<meta property="og:title" content=". In NeurlPS, 2025." />
  </head>

	
<body>
    <br>
          <center>
          	<!-- <span style="font-size:34px">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</span> -->
          	<span style="font-size:34px">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</span><br>
		  <br>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://lwpyh.github.io/">Jian Hu</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=120px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://zxccade.github.io/">Zixu Cheng</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=120px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a></span>
							</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=110px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://ece.hkust.edu.hk/eeguan">Isabel Guan</a></span>
							</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=FCJVUYgAAAAJ&hl=zh-CN">Jianye Hao</a></span>
							</a><sup>3</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a></span>
							</a><sup>4</sup></span>
		  		  		</center>
		  		  	  </td>
				<td align=center width=100px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://shaokun91.github.io/">Kun Shao</a></span>
							</a><sup>3</sup></span>
		  		  		</center>
		  		  		</td>
			  </table>
          	<span style="font-size:20px"><sup>1</sup>Queen Mary University of London, <sup>2</sup>Hong Kong University of Science and Technology, <sup>3</sup>Huawei Noah's Ark Lab, <sup>4</sup>University College London</span><br>
		  <span style="font-size:20px">{jian.hu, zixu.cheng, s.gong}@qmul.ac.uk, {jianye.hao,shaokun2}@huawei.com, jun.wang@cs.ucl.ac.uk</span>
          	<!-- <span style="font-size:22px">In CVPR, 2017.</span><br> -->
          		<!-- <span style="font-size:30px">ECCV 2016.</span> -->

	  		  <table align=center width=400px>
	  			  <tr>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:20px"><a href='https://github.com/lwpyh/URPO'> Code [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:20px"><a href='https://arxiv.org/abs/2508.06317'> NeurIPS 2025 [Paper]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>
  		  <br>

	<table align=center width=800px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "motivation_show.png" height="320px"></img>
  	                	<br>
						<figcaption>Motivation: The critical problem of how to select shots in video understanding. 
   In a video that depicts how a boy gradually gains a dragon's trust,
   different sampling methods create two distinct narratives: split
   video A shows the boy being attacked by the dragon, while split
   video B shows him happily sharing food with the dragon. 
  This shows that minor differences in video sampling leads to significant variations in semantic understanding.</figcaption>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					</center>
  	              </td>
  		  </table>

  		  <br>
            	  <hr>
<table align=center width=900px>
 <center><h1>Abstract</h1></center>
	<div align="justify">
	<p>
Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic
matching, they often struggle with fine-grained temporal
localisation. Recently, Group Relative Policy Optimisation (GRPO)
reformulates the inference process as a reinforcement learning
task, enabling fine-grained grounding and achieving strong in-domain
performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To
overcome both problems, we
introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding
method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of {\em unlabelled videos from the target domain}. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce
\textbf{U}ncertainty-quantified \textbf{R}ollout \textbf{P}olicy
\textbf{A}daptation (\textbf{URPA}) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using
GRPO rollouts, averages them to form a pseudo label, and estimates
confidence from the variance across these rollouts. This confidence
then weights the training rewards, guiding the model to focus on
reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. </p>
		</div>
<br>
<hr>
<!-- <hr> -->
<!-- <center><h1>Video</h1></center> -->
     <!-- <center>
     <iframe width="560" height="315" src="https://www.youtube.com/embed/oMcd6maQgQk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
     </center> -->
<!-- <br> -->

<center><h1>Framework</h1></center>
<table align="center" width="800px">
    <tr>
        <td width="400px">
            <center>
                <figure>
                    <img class="rounded" src="frame_CoS.png" height="260px" />
                    <figcaption style="text-align: justify;">
                            The overall framework of CoS. It first utilises LLaVA to
                            perform a mosaicing binary coding to bootstrap video
                            summarisation for temporal grounding on a long video. Specifically, every
                            four shots are aggregated into a mosaicing composition image. LLaVA
                            determines whether task-related elements exist within each
                            composition image by encoding a binary value of 1 or 0 (<code>'yes'</code>
                            or <code>'no'</code>),
                            thereby identifying sparsely distributed task-related shots to
                            achieve pseudo temporal grounding. Given this binary video
                            summary, task-related positive shots  \( S^p \) and irrelevant negative shots \( S^n \)
                            are generated and represented by binary codes. \( S^p \), \( S^n \) and the original frame sequence \( X \)
                            sampled from original video \( V \) are then fed into the MLLM for co-reasoning,
                            minimising interference of irrelevant video content.
</figcaption>
                </figure>
        <td width="400px">
        </td>
    </tr>
</table>
<hr>
<table align="center" width="1700px">

<center><h1>Experiments</h1></center>
<table align="center" width="800px style="margin-bottom: -10px;">
    <tr>
        <td width="400px">
            <center>
                <figure>
			<figcaption style="text-align: justify;">
				
    </figcaption>
		    <img class="rounded" src="videomme.png" height="600px" />
                </figure>
        <td width="400px">
        </td>
    </tr>
</table>
  <hr>
<table align="center" width="2800px">

<center><h1>Qualitative Evaluation</h1></center>
	<table align=center width="800px style="margin-bottom: 10px;">
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "image_sample.png" height="500px"></img>
  	                	<br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					</center>
  	              </td>
  		  </table>

  		  <br>
            	  <hr>
<table align=center width=1900px>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2025cos,
  title={CoS: Chain-of-Shot Prompting for Long Video Understanding},
  author={Hu, Jian and Cheng, Zixu and Si, Chenyang and Li, Wei and Gong, Shaogang},
  journal={arXiv preprint arXiv:2502.06428},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
           
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
	
</body>
</html>
